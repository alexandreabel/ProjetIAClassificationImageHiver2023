{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "tuto fait avec chatgpt sur la classification d'image pour apprendre les base."
      ],
      "metadata": {
        "id": "yqam7hNBDVTw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "pip install tensorflow keras opencv-python numpy matplotlib Pillow\n",
        "\n",
        "!pip install tensorflow==2.4.1 tensorflow-gpu==2.4.1 tensorflow-hub opencv-python matplotlib\n",
        "\n",
        "celui ci marche ac pycharm\n",
        "\n",
        "pip install tensorflow==2.4.1 tensorflow-gpu==2.4.1 keras opencv-python numpy matplotlib Pillow"
      ],
      "metadata": {
        "id": "cZQQnb6iDmA-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " #je serais ravi de vous aider à développer un modèle de classification d'image performant pour détecter des fruits en utilisant Tensorflow et Keras. Voici les étapes que nous pouvons suivre :\n",
        "\n",
        "# 1. Collecte des données : Tout d'abord, nous devons collecter des images de fruits pour entraîner notre modèle.\n",
        "# Nous pouvons rechercher des ensembles de données disponibles en ligne ou prendre nos propres photos de fruits.\n",
        "\n",
        "\n",
        "# 2. Prétraitement des données : Nous devons prétraiter les images avant de les utiliser pour l'entraînement du modèle.\n",
        "# Cela peut inclure le redimensionnement des images, la normalisation des couleurs et l'augmentation des données pour éviter la suradaptation.\n",
        "\n",
        "\n",
        "# 3. Séparation des données : Nous devons séparer les images en ensembles d'entraînement, de validation et de test.\n",
        "# L'ensemble d'entraînement sera utilisé pour entraîner le modèle, l'ensemble de validation sera utilisé pour évaluer les performances\n",
        "# du modèle pendant l'entraînement et l'ensemble de test sera utilisé pour évaluer les performances du modèle après l'entraînement.\n",
        "\n",
        "\n",
        "# 4. Création du modèle : Nous pouvons utiliser une architecture de réseau de neurones convolutionnel (CNN) pour classer les images de fruits.\n",
        "# Nous pouvons commencer avec une architecture simple et l'ajuster au fur et à mesure que nous progressons.\n",
        "\n",
        "\n",
        "# 5. Entraînement du modèle : Nous entraînerons notre modèle en utilisant l'ensemble d'entraînement et\n",
        "# en ajustant les poids du réseau pour minimiser l'erreur de classification.\n",
        "\n",
        "\n",
        "# 6. Évaluation du modèle : Nous évaluerons les performances de notre modèle en utilisant\n",
        "# l'ensemble de validation et en ajustant les hyperparamètres pour améliorer les performances.\n",
        "\n",
        "\n",
        "# 7. Test du modèle : Nous testerons finalement notre modèle sur l'ensemble de test\n",
        "# pour évaluer sa performance sur des données qu'il n'a pas vu auparavant.\n",
        "\n",
        "\n",
        "#Pour commencer, je vous recommande de commencer avec la détection de deux fruits et d'utiliser le jeu de données \"Fruits-360\" disponible en ligne.\n",
        "# Une fois que vous êtes à l'aise avec ce modèle, nous pouvons augmenter le nombre de fruits détectables jusqu'à 10."
      ],
      "metadata": {
        "id": "xImtQUM_D7Rz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#/////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n",
        "#DÉBUT ÉTAPE 2 PRÉTRAITEMENT DES DONNÉES\n",
        "#/////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////"
      ],
      "metadata": {
        "id": "HDoExVY5EBrm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Super, maintenant que vous avez téléchargé le jeu de données \"Fruits-360\", nous pouvons passer à la prochaine étape.\n",
        "# Voici les étapes à suivre pour prétraiter les données :\n",
        "\n",
        "# 1.Importez les bibliothèques Python nécessaires, y compris Tensorflow, Keras et\n",
        "# les bibliothèques de traitement d'image telles que Pillow ou OpenCV.\n",
        "\n",
        "# 2.Chargez les images de fruits à partir du dossier où vous avez téléchargé le jeu de données \"Fruits-360\".\n",
        "# Vous pouvez utiliser une boucle pour charger toutes les images dans un répertoire et les stocker dans une liste ou un tableau.\n",
        "\n",
        "# 3.Étiquetez chaque image avec le nom de son fruit correspondant. Comme mentionné précédemment, vous pouvez utiliser\n",
        "# des dictionnaires Python pour associer les noms de fruits à des entiers et convertir ces entiers en vecteurs one-hot.\n",
        "\n",
        "# 4.Divisez les données en ensembles d'entraînement, de validation et de test. Vous pouvez utiliser la fonction train_test_split de\n",
        "# la bibliothèque Scikit-learn pour séparer les données en ensembles d'entraînement et de test. Vous pouvez également diviser l'ensemble\n",
        "# d'entraînement en ensembles d'entraînement et de validation en utilisant la même fonction.\n",
        "\n",
        "# 5.Prétraitez les images en les redimensionnant à une taille fixe, en normalisant les couleurs et en appliquant des techniques\n",
        "# d'augmentation de données telles que la rotation, le zoom et le basculement pour augmenter le nombre d'images dans l'ensemble d'entraînement.\n",
        "\n",
        "# 6.Convertissez les images en tenseurs et préparez-les pour l'entraînement du modèle en les redimensionnant et en les normalisant.\n",
        "\n",
        "#Une fois que les données sont prétraitées, nous pouvons passer à la création du modèle CNN pour la classification d'images.\n",
        "# Dites-moi si vous avez des questions ou si vous avez besoin d'aide pour prétraiter les données."
      ],
      "metadata": {
        "id": "GIRKmJiJEG8t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#SECTION 1 Importez les bibliothèques Python nécessaires"
      ],
      "metadata": {
        "id": "RUFK_HQFEM_Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install labels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6UJCZDblIzir",
        "outputId": "0b3e3a41-4854-4f97-a56c-de793ddd3862"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting labels\n",
            "  Downloading labels-20.1.0-py3-none-any.whl (10 kB)\n",
            "Collecting pytoml\n",
            "  Downloading pytoml-0.1.21-py2.py3-none-any.whl (8.5 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from labels) (2.25.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from labels) (8.1.3)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.8/dist-packages (from labels) (22.2.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->labels) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->labels) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->labels) (1.26.14)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->labels) (2022.12.7)\n",
            "Installing collected packages: pytoml, labels\n",
            "Successfully installed labels-20.1.0 pytoml-0.1.21\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-lu5EadGDH1u"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from sklearn.datasets import images\n",
        "from tensorflow import keras\n",
        "#from tensorflow.image import ImageDataGenerator\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import PIL\n",
        "from PIL import Image\n",
        "import cv2\n",
        "import labels as labels"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#SECTION 2 Chargez les images de fruits à partir du dossier où vous avez téléchargé le jeu de données \"Fruits-360\"."
      ],
      "metadata": {
        "id": "RIMjs21KERaM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dir = \"fruits-360/Training\"\n",
        "batch_size = 32\n",
        "img_height = 100\n",
        "img_width = 100\n",
        "\n",
        "train_data = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    validation_split=0.2) # 20% des images pour la validation\n",
        "\n",
        "train_dataset = train_data.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    subset='training',\n",
        "    shuffle=True,\n",
        "    seed=42)\n",
        "\n",
        "validation_dataset = train_data.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    subset='validation',\n",
        "    shuffle=True,\n",
        "    seed=42)"
      ],
      "metadata": {
        "id": "o9EtCIc0GV1E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Explications :\n",
        "\n",
        " 'train_dir' est le chemin du dossier contenant les images de formation.\n",
        " 'batch_size' est la taille du lot pour le traitement des images.\n",
        " 'img_height' et 'img_width' sont les dimensions cibles de l'image.\n",
        " 'ImageDataGenerator' permet de prétraiter les images en redimensionnant les images à une taille fixe, en normalisant les valeurs de pixel, en appliquant des transformations d'augmentation de données telles que la rotation, le zoom et le retournement horizontal.\n",
        " 'flow_from_directory' permet de charger les images à partir du dossier de formation et de les diviser en ensembles de formation et de validation, en utilisant un fractionnement de 80/20, respectivement. Les images sont chargées en lots de la taille spécifiée par batch_size. Les images sont chargées avec la classe cible categorical car c'est un problème de classification multi-classe.\n",
        "\n"
      ],
      "metadata": {
        "id": "3NtFkINwGY1v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_dir = \"fruits-360/Test\"\n",
        "\n",
        "test_dataset = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "    rescale=1./255).flow_from_directory(\n",
        "    test_dir,\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    shuffle=False)"
      ],
      "metadata": {
        "id": "sxSd-3oEGdEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vous pouvez également ajouter un 'test_dataset' en utilisant 'flow_from_directory' de la même manière pour charger les images de test\n",
        " à partir du dossier \"fruits-360/Test\". Cela vous permettra d'évaluer les performances du modèle sur des données qu'il n'a jamais vues auparavant."
      ],
      "metadata": {
        "id": "ypEnGiFhGgNk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#/////////////////////////////////////////////////////////////////////////////////////////\n",
        "#SECTION 3. Étiquetez chaque image avec le nom de son fruit correspondant.\n",
        "#/////////////////////////////////////////////////////////////////////////////////////////"
      ],
      "metadata": {
        "id": "jBNolLBEGlo2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pour étiqueter chaque image avec le nom de son fruit correspondant, vous pouvez\n",
        " utiliser la méthode \"flow_from_directory\" de la classe \"ImageDataGenerator\" de Keras.\n",
        "\n",
        "Cette méthode permet de charger les images directement à partir des dossiers,\n",
        " tout en les étiquetant automatiquement avec les noms des sous-dossiers. Voici un exemple de code"
      ],
      "metadata": {
        "id": "m1fPGhyjGwtR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_generator = train_data.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical'\n",
        ")"
      ],
      "metadata": {
        "id": "nzg0jbVAGzIw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dans cet exemple, \"train_dir\" est le chemin vers le dossier contenant les images d'entraînement,\n",
        " \"img_height\" et \"img_width\" sont les dimensions cibles des images, \"batch_size\" est la taille du lot (batch_size),\n",
        " et class_mode est défini sur \"categorical\" pour étiqueter automatiquement chaque image avec le nom de son dossier parent.\n",
        "\n",
        "Après avoir chargé les images à partir des dossiers et les avoir étiquetées avec les noms de fruits correspondants,\n",
        " vous pouvez commencer à entraîner le modèle de classification d'images."
      ],
      "metadata": {
        "id": "hce7-gfwG3rw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#/////////////////////////////////////////////////////////////////////////////////////////\n",
        "#SECTION 4 Divisez les données en ensembles d'entraînement, de validation et de test\n",
        "#/////////////////////////////////////////////////////////////////////////////////////////"
      ],
      "metadata": {
        "id": "zNOTFFGRG8JD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Dans cette section, l'objectif est de diviser les données en ensembles distincts pour l'entraînement,\n",
        " la validation et le test. Cela permet de mesurer l'exactitude du modèle sur des données\n",
        " qu'il n'a jamais vues auparavant et d'éviter le surapprentissage.\n",
        "\n",
        "Voici comment vous pouvez diviser les données en utilisant la méthode \"flow_from_directory\""
      ],
      "metadata": {
        "id": "wmoD6FBfG-7G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = train_data.flow_from_directory(\n",
        "        directory=train_dir,\n",
        "        target_size=(img_height, img_width),\n",
        "        batch_size=batch_size,\n",
        "        class_mode='categorical',\n",
        "        shuffle=True,\n",
        "        seed=42)\n",
        "\n",
        "validation_data = train_data.flow_from_directory(\n",
        "        directory=validation_dataset,\n",
        "        target_size=(img_height, img_width),\n",
        "        batch_size=batch_size,\n",
        "        class_mode='categorical',\n",
        "        shuffle=False,\n",
        "        seed=42)\n",
        "\n",
        "test_data = train_data.flow_from_directory(\n",
        "        directory=test_dir,\n",
        "        target_size=(img_height, img_width),\n",
        "        batch_size=batch_size,\n",
        "        class_mode='categorical',\n",
        "        shuffle=False,\n",
        "        seed=42)"
      ],
      "metadata": {
        "id": "ByEjnkbVHjxz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vous devez spécifier le chemin d'accès aux répertoires d'entraînement,\n",
        " de validation et de test. La méthode flow_from_directory prend également en compte la taille cible des images,\n",
        " la taille du lot, le mode de classe et le paramètre de mélange pour chaque ensemble de données.\n",
        "\n",
        "Il est important de noter que les ensembles de données d'entraînement et de validation sont mélangés (shuffle=True),\n",
        " tandis que le jeu de données de test n'est pas mélangé (shuffle=False). Cela permet de garantir que les données\n",
        " de test sont utilisées pour évaluer la performance du modèle sur des données qu'il n'a jamais vues auparavant.\n",
        "\n",
        "Enfin, il est important de s'assurer que les données d'entraînement, de validation et de test sont toutes\n",
        " représentatives de la distribution des données réelles. Il est recommandé de choisir une répartition des données qui\n",
        " permette de maximiser la quantité de données d'entraînement, tout en maintenant suffisamment de données de validation et\n",
        " de test pour mesurer l'exactitude du modèle de manière fiable."
      ],
      "metadata": {
        "id": "xbvRmOKEHm3K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#/////////////////////////////////////////////////////////////////////////////////////////\n",
        "#SECTION 5 Prétraitez les images en les redimensionnant à une taille fixe, en normalisant les couleurs et en appliquant des techniques d'augmentation de données telles que la rotation, le zoom et le basculement pour augmenter le nombre d'images dans l'ensemble d'entraînement.\n",
        "#/////////////////////////////////////////////////////////////////////////////////////////"
      ],
      "metadata": {
        "id": "GTfx9imGHv2b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "pour prétraiter les images, nous allons utiliser la classe \"ImageDataGenerator\" de TensorFlow.\n",
        " Cette classe permet de charger des images depuis le disque et de les prétraiter à la volée pendant l'entraînement du modèle.\n",
        "\n",
        "Voici un exemple de code pour prétraiter les images en redimensionnant à une taille fixe de 100x100 pixels,\n",
        " en normalisant les valeurs de pixel pour qu'elles soient dans la plage [0,1] et en appliquant des transformations d'augmentation de\n",
        " données aléatoires comme la rotation, le zoom et le basculement"
      ],
      "metadata": {
        "id": "7QHHJtjSHyuq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,  # Normalisation des valeurs de pixel pour qu'elles soient dans la plage [0,1]\n",
        "    rotation_range=40,  # Rotation aléatoire des images dans une plage de 40 degrés\n",
        "    width_shift_range=0.2,  # Translation horizontale aléatoire des images dans une plage de 20% de la largeur de l'image\n",
        "    height_shift_range=0.2,  # Translation verticale aléatoire des images dans une plage de 20% de la hauteur de l'image\n",
        "    shear_range=0.2,  # Transformation de cisaillement aléatoire\n",
        "    zoom_range=0.2,  # Zoom aléatoire dans une plage de 20%\n",
        "    horizontal_flip=True,  # Retournement horizontal aléatoire des images\n",
        "    fill_mode='nearest'  # Mode de remplissage pour les pixels nouvellement créés\n",
        ")\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)  # Pour le set de test, nous ne faisons que la normalisation des valeurs de pixel\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=(100, 100),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "validation_generator = test_datagen.flow_from_directory(\n",
        "    validation_dataset,\n",
        "    target_size=(100, 100),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "W9BioQkoH3eD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dans cet exemple, nous avons défini deux instances de \"ImageDataGenerator\" : \"train_datagen\" et \"test_datagen\".\n",
        " Nous avons ensuite configuré \"train_datagen\" pour effectuer les transformations d'augmentation de données aléatoires.\n",
        " Nous avons également défini \"test_datagen\" pour effectuer la normalisation des valeurs de pixel uniquement.\n",
        "\n",
        "Enfin, nous avons utilisé les méthodes \"flow_from_directory()\" de chaque instance pour créer des générateurs d'images pour l'ensemble\n",
        " d'entraînement et l'ensemble de validation, respectivement. Ces générateurs chargent les images à partir des répertoires d'images et appliquent les transformations appropriées à la volée.\n",
        "\n",
        "Notez que les arguments \"target_size\" et \"class_mode\" doivent être définis pour que les générateurs fonctionnent correctement.\n",
        " Le premier argument spécifie la taille de l'image que nous voulons pour l'entrée du modèle, et le deuxième argument\n",
        " spécifie le mode de classification que nous utilisons, qui est \"categorical\" dans notre cas.\n",
        "\n"
      ],
      "metadata": {
        "id": "kgLJeoE1H6vr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# étape 6. Convertissez les images en tenseurs et préparez-les pour l'entraînement du modèle en les redimensionnant et en les normalisant."
      ],
      "metadata": {
        "id": "1WuHgsKyH_Cb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    validation_split=0.2) # 20% des images pour la validation\n",
        "\n",
        "train_dataset = train_data.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    subset='training',\n",
        "    shuffle=True,\n",
        "    seed=42)\n",
        "\n",
        "validation_dataset = train_data.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    subset='validation',\n",
        "    shuffle=True,\n",
        "    seed=42)\n",
        "\n",
        "test_dataset = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "    rescale=1./255).flow_from_directory(\n",
        "    test_dir,\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    shuffle=False)"
      ],
      "metadata": {
        "id": "XeAGmftIH78n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#FIN ÉTAPE 2 PRÉTRAITEMENT DES DONNÉES"
      ],
      "metadata": {
        "id": "GHbiqz8XIF1j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#DÉBUT ÉTAPE 3 Séparation des données"
      ],
      "metadata": {
        "id": "wVnu_PEyIGs6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pip install scikit-learn\n",
        "#pt  pip install sklearn.datasets.images\n",
        "#pt  pip install labels"
      ],
      "metadata": {
        "id": "KfLRUlW7ILYS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Diviser les données en ensembles d'entraînement et de test\n",
        "X_train, X_test, y_train, y_test = train_test_split(images, labels, test_size=0.15, random_state=42) #tu a installé le package images ou sklearn.datasets.images (cher po trop) et labels avec le\n",
        "\n",
        "# Diviser l'ensemble d'entraînement en ensembles d'entraînement et de validation\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.15, random_state=42)"
      ],
      "metadata": {
        "id": "3estHWnbIUxh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#FIN ÉTAPE 3 Séparation des données"
      ],
      "metadata": {
        "id": "eis5-LPYIXCL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#DÉBUT ÉTAPE 4"
      ],
      "metadata": {
        "id": "rOtO7_9wIb1G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Dense, Flatten\n",
        "\n",
        "num_classes = 10\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(100, 100, 3)))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(num_classes, activation='softmax'))"
      ],
      "metadata": {
        "id": "3n-PO_f2IgLY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Voici le code pour compiler le modèle"
      ],
      "metadata": {
        "id": "rFD4Yp2sTsYB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "67sU8w_3TPQl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Voici le code pour entraîner notre modèle"
      ],
      "metadata": {
        "id": "XANNCWgATvO2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(train_generator, epochs=10, validation_data=val_generator)"
      ],
      "metadata": {
        "id": "VkjcKzIXTR-L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Voici le code pour évaluer la performance de notre modèle sur les données de test"
      ],
      "metadata": {
        "id": "RyTXS6OgT1st"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_acc = model.evaluate(test_generator)\n",
        "print('Test accuracy:', test_acc)"
      ],
      "metadata": {
        "id": "TAeRIi4lTXd8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Voici le code pour classer de nouvelles images avec notre modèle"
      ],
      "metadata": {
        "id": "wXqfpDrHT8ee"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = model.predict(new_images)\n",
        "print(predictions)"
      ],
      "metadata": {
        "id": "Lf7IRftfTbPR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}